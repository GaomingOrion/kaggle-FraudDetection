{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc, re, sys, os, datetime, pickle\n",
    "sys.path.append('..')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "SEED = 42\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':80000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':100,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(train_df, test_df, columns, self_encoding=False):\n",
    "    for col in columns:\n",
    "        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n",
    "        if self_encoding:\n",
    "            train_df[col] = train_df[col].map(fq_encode)\n",
    "            test_df[col]  = test_df[col].map(fq_encode)            \n",
    "        else:\n",
    "            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n",
    "                                 with_proportions=True, only_proportions=False):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n",
    "            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts().to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
    "            \n",
    "            if only_proportions:\n",
    "                train_df[new_col] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col]  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "            if with_proportions:\n",
    "                train_df[new_col+'_proportions'] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col+'_proportions']  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def uid_aggregation_and_normalization(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            \n",
    "            new_norm_col_name = col+'_'+main_column+'_std_norm'\n",
    "            norm_cols = []\n",
    "            \n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "                norm_cols.append(new_col_name)\n",
    "            \n",
    "            train_df[new_norm_col_name] = (train_df[main_column]-train_df[norm_cols[0]])/train_df[norm_cols[1]]\n",
    "            test_df[new_norm_col_name]  = (test_df[main_column]-test_df[norm_cols[0]])/test_df[norm_cols[1]]          \n",
    "            \n",
    "            del train_df[norm_cols[0]], train_df[norm_cols[1]]\n",
    "            del test_df[norm_cols[0]], test_df[norm_cols[1]]\n",
    "                                              \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cor_and_remove(train_df, test_df, i_cols, new_columns, remove=False):\n",
    "    # Check correllation\n",
    "    print('Correlations','#'*10)\n",
    "    for col in new_columns:\n",
    "        cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
    "        print(col, cor_cof)\n",
    "\n",
    "    if remove:\n",
    "        print('#'*10)\n",
    "        print('Best options:')\n",
    "        best_fe_columns = []\n",
    "        for main_col in i_cols:\n",
    "            best_option = ''\n",
    "            best_cof = 0\n",
    "            for col in new_columns:\n",
    "                if main_col in col:\n",
    "                    cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
    "                    cor_cof = (cor_cof**2)**0.5\n",
    "                    if cor_cof>best_cof:\n",
    "                        best_cof = cor_cof\n",
    "                        best_option = col\n",
    "\n",
    "            print(main_col, best_option, best_cof)            \n",
    "            best_fe_columns.append(best_option)\n",
    "\n",
    "        for col in new_columns:\n",
    "            if col not in best_fe_columns:\n",
    "                del train_df[col], test_df[col]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train_transaction.csv')\n",
    "test_df = pd.read_csv('../input/test_transaction.csv')\n",
    "test_df['isFraud'] = 0\n",
    "\n",
    "train_identity = pd.read_csv('../input/train_identity.csv')\n",
    "test_identity = pd.read_csv('../input/test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
      "Mem. usage decreased to 473.07 Mb (68.9% reduction)\n",
      "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n",
      "Mem. usage decreased to 25.44 Mb (42.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)\n",
    "\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding card4\n",
      "{'visa': 719649, 'mastercard': 347386, 'american express': 16009, 'discover': 9524}\n",
      "Encoding card6\n",
      "{'debit': 824959, 'credit': 267648, 'debit or credit': 30, 'charge card': 16}\n",
      "Encoding ProductCD\n",
      "{'W': 800657, 'C': 137785, 'R': 73346, 'H': 62397, 'S': 23046}\n"
     ]
    }
   ],
   "source": [
    "for col in ['card4', 'card6', 'ProductCD']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding M4\n",
      "{'M0': 357789, 'M2': 122947, 'M1': 97306}\n"
     ]
    }
   ],
   "source": [
    "for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
    "    train_df[col] = train_df[col].map({'T':1, 'F':0})\n",
    "    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n",
    "\n",
    "for col in ['M4']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def minify_identity_df(df):\n",
    "\n",
    "    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
    "    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
    "\n",
    "    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n",
    "\n",
    "    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n",
    "    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n",
    "    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n",
    "    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n",
    "\n",
    "    df['id_34'] = df['id_34'].fillna(':0')\n",
    "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n",
    "    \n",
    "    df['id_33'] = df['id_33'].fillna('0x0')\n",
    "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n",
    "\n",
    "    df['DeviceType'].map({'desktop':1, 'mobile':0})\n",
    "    return df\n",
    "\n",
    "train_identity = minify_identity_df(train_identity)\n",
    "test_identity = minify_identity_df(test_identity)\n",
    "\n",
    "for col in ['id_33']:\n",
    "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
    "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_identity[col])+list(test_identity[col]))\n",
    "    train_identity[col] = le.transform(train_identity[col])\n",
    "    test_identity[col]  = le.transform(test_identity[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 15.54 Mb (41.1% reduction)\n",
      "Mem. usage decreased to 15.29 Mb (41.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('../input/train_transaction.pkl')\n",
    "test_df.to_pickle('../input/test_transaction.pkl')\n",
    "\n",
    "train_identity.to_pickle('../input/train_identity.pkl')\n",
    "test_identity.to_pickle('../input/test_identity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>card1</th>\n",
       "      <th>Date</th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.424</td>\n",
       "      <td>C</td>\n",
       "      <td>305.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>12616</td>\n",
       "      <td>2017-12-12 19:47:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424</td>\n",
       "      <td>C</td>\n",
       "      <td>305.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>12616</td>\n",
       "      <td>2017-12-12 19:53:03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.424</td>\n",
       "      <td>C</td>\n",
       "      <td>305.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>12616</td>\n",
       "      <td>2017-12-12 20:38:19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424</td>\n",
       "      <td>C</td>\n",
       "      <td>305.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>12616</td>\n",
       "      <td>2017-12-12 20:45:59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.467</td>\n",
       "      <td>C</td>\n",
       "      <td>305.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>12616</td>\n",
       "      <td>2017-12-12 20:37:23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432663</td>\n",
       "      <td>5420.000</td>\n",
       "      <td>W</td>\n",
       "      <td>123.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>5033</td>\n",
       "      <td>2018-02-16 14:52:36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432664</td>\n",
       "      <td>5543.230</td>\n",
       "      <td>W</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>msn.com</td>\n",
       "      <td>11106</td>\n",
       "      <td>2018-03-22 16:15:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432665</td>\n",
       "      <td>6450.970</td>\n",
       "      <td>W</td>\n",
       "      <td>327.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>16661</td>\n",
       "      <td>2018-02-23 17:23:08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432666</td>\n",
       "      <td>31937.391</td>\n",
       "      <td>W</td>\n",
       "      <td>205.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>16075</td>\n",
       "      <td>2018-02-15 23:52:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432667</td>\n",
       "      <td>31937.391</td>\n",
       "      <td>W</td>\n",
       "      <td>205.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>16075</td>\n",
       "      <td>2018-02-15 23:53:11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432668 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionAmt ProductCD  addr1  addr2 P_emaildomain  card1  \\\n",
       "0                0.424         C  305.0   60.0   hotmail.com  12616   \n",
       "1                0.424         C  305.0   60.0   hotmail.com  12616   \n",
       "2                0.424         C  305.0   60.0   hotmail.com  12616   \n",
       "3                0.424         C  305.0   60.0   hotmail.com  12616   \n",
       "4                0.467         C  305.0   60.0   hotmail.com  12616   \n",
       "...                ...       ...    ...    ...           ...    ...   \n",
       "432663        5420.000         W  123.0   87.0     gmail.com   5033   \n",
       "432664        5543.230         W  315.0   87.0       msn.com  11106   \n",
       "432665        6450.970         W  327.0   87.0     yahoo.com  16661   \n",
       "432666       31937.391         W  205.0   87.0     yahoo.com  16075   \n",
       "432667       31937.391         W  205.0   87.0     yahoo.com  16075   \n",
       "\n",
       "                      Date  count  sum  \n",
       "0      2017-12-12 19:47:50    1.0  1.0  \n",
       "1      2017-12-12 19:53:03    2.0  2.0  \n",
       "2      2017-12-12 20:38:19    3.0  3.0  \n",
       "3      2017-12-12 20:45:59    4.0  4.0  \n",
       "4      2017-12-12 20:37:23    1.0  1.0  \n",
       "...                    ...    ...  ...  \n",
       "432663 2018-02-16 14:52:36    2.0  0.0  \n",
       "432664 2018-03-22 16:15:12    1.0  0.0  \n",
       "432665 2018-02-23 17:23:08    1.0  0.0  \n",
       "432666 2018-02-15 23:52:40    1.0  0.0  \n",
       "432667 2018-02-15 23:53:11    3.0  0.0  \n",
       "\n",
       "[432668 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rolling_func(df, id_cols, date_col, label, offset):\n",
    "    MAX_DATE = pd.to_datetime('2020-01-01 00:00:00')\n",
    "    date_col_rev = date_col+'#tmp'\n",
    "    df[date_col_rev] = MAX_DATE - df[date_col]\n",
    "    agg = ['count', 'sum']\n",
    "    before = df.set_index(date_col).groupby(id_cols)[label].rolling(offset, closed='left').agg(agg).fillna(0).reset_index()\n",
    "    after = df.set_index(date_col_rev).groupby(id_cols)[label].rolling(offset, closed='both').agg(agg).reset_index()\n",
    "    df.drop(columns=date_col_rev, inplace=True)\n",
    "    before[agg] = before[agg].values + after[agg].values\n",
    "    return before\n",
    "id_cols = [ 'TransactionAmt', 'ProductCD','addr1', 'addr2', 'P_emaildomain', 'card1']\n",
    "a = rolling_func(df_train, id_cols, 'Date', 'isFraud', '60s')\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
